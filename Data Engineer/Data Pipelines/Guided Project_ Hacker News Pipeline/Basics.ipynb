{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "In this course, we began with the concepts of functional programming, and then built our own data pipeline class in Python. We learned about advanced Python concepts such as the decorators, closures, and good API design. In the last mission, we also learned how to implement a directed acyclic graph as the scheduler for our pipeline.\n",
    "\n",
    "After completing all these missions, we have finally built a robust data pipeline that schedules our tasks in the correct order! In this guided project, we will use the pipeline we have been building, and apply it to a real world data pipeline project. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal will be to find the top 100 keywords of Hacker News posts in 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data\n",
    "\n",
    "Instantiate an instance of the Pipeline class and assign it to the variable \"pipeline.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data\n",
    "\n",
    "* Create a pipeline.task() function that takes in no arguments.\n",
    "* Call the function file_to_json(), where the function does the following:\n",
    "    * Loads the hn_stories_2014.json file into a Python dict.\n",
    "    * Returns the list of stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories\n",
    "\n",
    "* Create a pipeline.task() function that depends on the file_to_json() function.\n",
    "* Call the new function filter_stories(), that filters popular stories that have more than 50 points, more than 1 comment, and do not begin with Ask HN.\n",
    "* filter_stories() should return a generator of these filtered stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    filtered = [x for x in stories if x[\"points\"]>50 and x[\"num_comments\"]>1 and not x['title'].startswith('Ask HN')]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "\n",
    "With a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of your pipeline tasks will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pipeline import build_csv\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    titles = [line[idx] for line in reader]\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles\n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. \n",
    "\n",
    "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    cleaned = []\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        cleaned.append(title)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary\n",
    "\n",
    "With a cleaned title, we can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import stop_words\n",
    "\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(cleaned):\n",
    "    word_freq = {}\n",
    "    for title in cleaned:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Top Words\n",
    "\n",
    "Finally, we're ready to sort the top words used in all the titles. In this final task, it's up to you to decide how you want to sort the top words. The goal is to output a list of tuples with (word, frequency) as the entries sorted from most used, to least most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_words(word_freq):\n",
    "    freq_tuple = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq_tuple[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('2014', 53), ('software', 53), ('startup', 52), ('use', 51), ('make', 51), ('apple', 51), ('security', 49), ('time', 49), ('yc', 49), ('github', 46), ('nsa', 46), ('windows', 45), ('like', 42), ('way', 42), ('world', 42), ('computer', 41), ('1', 41), ('project', 41), ('heartbleed', 41), ('users', 38), ('dont', 38), ('design', 38), ('git', 38), ('ios', 38), ('ceo', 37), ('twitter', 37), ('os', 37), ('developer', 37), ('vs', 37), ('life', 37), ('day', 36), ('big', 36), ('android', 35), ('online', 35), ('simple', 34), ('court', 34), ('years', 34), ('api', 33), ('mt', 33), ('learning', 33), ('guide', 33), ('says', 33), ('browser', 33), ('apps', 33), ('fast', 32), ('site', 32), ('engine', 32), ('problem', 32), ('firefox', 32), ('server', 32), ('mozilla', 32), ('gox', 32), ('introducing', 31), ('year', 31), ('amazon', 31), ('built', 30), ('text', 30), ('better', 30), ('million', 30), ('people', 30), ('stop', 30), ('support', 30), ('tech', 29), ('3', 29), ('development', 29), ('does', 29), ('did', 28), ('money', 28), ('just', 28), ('inside', 28), ('2048', 28), ('library', 28), ('website', 28), ('chrome', 28)]\n"
     ]
    }
   ],
   "source": [
    "ran = pipeline.run()\n",
    "print(ran[top_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
